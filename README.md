![logo](docs/finabyss_dark.svg)

FinABYSS (Financial Aspect-Based Hybrid Semantic System)
---
- [üåè Semantic Map](#-semantic-map)
  - [üíª Installation](#-installation)
  - [üõ† How to Use](#-how-to-use)
- [‚öôÔ∏è Architecture](#Ô∏è-architecture)
- [‚≠êÔ∏è Key Features](#Ô∏è-key-features)
  - [üåÄ Local \& Global Structure](#-local--global-structure)
  - [üìö Long Context](#-long-context)
  - [üöÄ Speed up](#-speed-up)
- [Corpus of financial news articles](#corpus-of-financial-news-articles)
- [‚úçÔ∏è Notes](#Ô∏è-notes)
  - [‚ùóÔ∏è Key Dependencies](#Ô∏è-key-dependencies)
  - [üå≥ Project Structure](#-project-structure)
  - [üöß Future Works](#-future-works)
  - [üìû Contacts](#-contacts)


This project aims to review the existing approach to financial forecasting and analysis, taking a step towards the analysis of multimodal data. We offer 2 key concepts:
1. Aspect-based sentiment analysis (ABSA), in the context of which aspects are considered as topics;
2. Considering financial sentiment as the strength and direction of information's influence on the price of a particular asset, rather than as an emotional coloring.

These assumptions have 3 key implications:
1. each document is represented by a set of topics from a predefined set without human involvement;
2. each publication has $k$ sentiments corresponding to each topic;
3. sentiments of one publication will vary depending on the asset.

# üåè Semantic Map
–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –∫–∞—Ä—Ç–∞ –º–æ–∂–µ—Ç —Å—Ç–∞—Ç—å –Ω–µ–æ—Ç—ä–µ–º–ª–µ–º–æ–π —á–∞—Å—Ç—å—é —Ä–∞–±–æ—á–µ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ –∞–Ω–∞–ª–∏—Ç–∏–∫–∞, –∏–Ω–≤–µ—Å—Ç–æ—Ä–∞ –∏–ª–∏ –∫–æ–≥–æ-—É–≥–æ–¥–Ω–æ –µ—â–µ, –∏–Ω—Ç–µ—Ä–µ—Å—É—é—â–µ–≥–æ—Å—è —Ñ–∏–Ω–∞–Ω—Å–∞–º–∏.

## üíª Installation
1. Just download [HTML file](semantic_map.html), right-click and open it in Microsoft Edge or Google Chrome (both gives the fastest response).
2. First, open the web-page and wait for it to fully load. You can determine whether the system is fully booted or not by the **pop-up windows when you hover over the dots**. If they pop up, the system is ready to function.
3. When the loading is complete, press the SHIFT key once. This is to speed up zooming in/out of the camera.

## üõ† How to Use
* The map shows the **Dots**, each one a financial news article:
  * the **size** of the dot reflects the number of characters in the article;
  * the **color** of the dot corresponds to the main topic of the article.
  * when you hover over a dot, a **pop-up window** appears, containing the title of the news article, the tickers to which the article belongs, the original source of the article, as well as the time and date of publication;
  * **clicking** on the dot redirects to the original web-page of the article.
* A **Search Bar** is available in the upper right corner below the logo, implementing search functionality. The search is performed on all texts of articles.
* A **Histogram** is available in the lower right corner, which realizes the possibility of filtering news by time range. Each bar is a month. When you hover over a bar, all articles for the month are highlighted. To select the time range, you should click on the desired initial bar and drag the mouse to the expected end of the period.
* Along the left side is the **Filter Menu**. Cross-interaction of filters by three categories is allowed: Source, Topic, and Ticker. User can check the boxes of sources, topics and tickers that the user wants to find:
  * Since there are quite a few items in each category, a **Search Box** is available in each category.
  * After selecting items, the corresponding names are placed under the Search Box, from where they **can later be removed** by clicking on the cross.
* Semantic Map also offers functionality to build a **Word Cloud** for any group of articles. The word cloud is constructed from the texts of the highlighted articles. The user can select an arbitrary group of articles by pressing the SHIFT key and starting to circle the area of interest. After selecting the objects, the Word Cloud will appear on the left side of the Map. Once the Word Cloud is built, the user can immediately select another group. **To delete a Word Cloud, you must reset all filters and press the SHIFT key once**.

![overview](docs/overview.gif)

# ‚öôÔ∏è Architecture

–î–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –≤–æ–≤—Å–µ –Ω–µ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç—Å—è –ª–∏—à—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –∫–∞—Ä—Ç–æ–π, –∫–æ—Ç–æ—Ä–∞—è –Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∏–Ω—Ç–µ–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –∫ –±–æ–ª–µ–µ –∑–∞–∫—Ä—ã—Ç–æ–º—É –ø—Ä–æ—Ü–µ—Å—Å—É ‚Äî –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—é —Å—Ç–æ–∏–º–æ—Å—Ç–∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –∞–∫—Ç–∏–≤–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Ü–µ–Ω–æ–∫ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–µ–π.

![architecture](docs/architecture.png)

![architecture_current_state](docs/architecture_current_state.png)

# ‚≠êÔ∏è Key Features
## üåÄ Local & Global Structure
This map preserves the semantic relationship of both the clusters and the texts themselves to each other quite well. In the problem of dimensionality reduction, the **local structure refers to the dots and determines how accurately they are located relative to the nearest dots**. This absolutely true for our Semantic Map. Let's take a look at an example related to an Electric Vehicles cluster on a Semantic Map. After we have selected this cluster, we can see that it is somewhat fragmented, that is, it consists of several microclusters scattered over different parts of the map.

![local_structure](docs/local_structure.gif)

The subcluster in the lower right corner contains articles more about the safety of autonomous transport. If we look at the cluster on the upper right side of the map, we can find that it's more about Europe vehicles industry. The same can be observed for the Cultural Tourism cluster, but in a more restrained form. Thus, mainland and island China can be observed in the upper part of the cluster, the Middle East from the bottom left, and South Asia on the bend.

Little differance lays in global structure that always determines as a represented variance of data. In our case, the **global structure mostly means the ability to continuously reflect a hierarchical structure**, that is, meso- and macro-clusters. It can be observed that such topical clusters as Hospitality Industry, Restaurant Industry, Alcohol Industry, Cannabis Industry, Cultural Tourism are quite close to each other, in fact forming the extended HoReCa industry.

![global_structure](docs/global_structure.gif)

The same can be observed with the clusters of National Security, Cryptocurrency Regulation and Cybersecurity.

So, on the Semantic Map, one can find some rather entertaining connections. For example, there is an area where the Electric Vehicle cluster is adjacent to the Mining Exploration, which we have been considering recently. If the word "lithium" is quite obvious, "gold" may surprise an unloaded user, however, the fact is that many times more gold is used for the production of electric cars than for cars with internal combustion engines. That is, the Semantic Map allows users who are not immersed in the specifics to discover rather deep inter-topic patterns.

![golden_finding](docs/golden_finding.png)

Another version of the Semantic Map is also available (PaCMAP-based), which, unlike the current one, prioritizes the global structure, but makes it difficult to trace local relationships.

## üìö Long Context

Previously, the `BERT` model or its improved versions were most commonly used for embedding tasks, but all were limited to contexts of 512 tokens, which did not allow processing long texts. Sliding contex window type methods are costly and lose long term contextual relationships.

This project focuses on "budget" LMs designed to extract embeddings from long texts. The current solution is based on [`gte-modernbert-base`](https://huggingface.co/Alibaba-NLP/gte-modernbert-base), which can handle up to 8192 tokens. However, this is still not enough, so in the near future it is planned to switch to the newly released [`Qwen3-Embedding-4B`](https://huggingface.co/Qwen/Qwen3-Embedding-4B), which is capable of processing up to 32k tokens.

Thus, the current solution allows us to reach a **new level** ‚Äî to process not only short posts in social networks and news headlines, but also press releases, news, transcripts of financial events and much more.

## üöÄ Speed up
* Embedding Extraction ‚Äî 9 times
* Models training (UMAP & HDBSCAN) ‚Äî 80 times

# Corpus of financial news articles
–î–∞—Ç–∞—Å–µ—Ç —Å–æ –≤—Å–µ–º–∏ —Å—Ç–∞—Ç—å—è–º–∏ —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω –≤ [—Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏](https://huggingface.co/datasets/denisalpino/YahooFinanceNewsRaw) –Ω–∞ HuggingFace.

# ‚úçÔ∏è Notes

## ‚ùóÔ∏è Key Dependencies

To reproduce the experiments, you need to create `Python 3.10` environment in which all the dependencies from `requirements.txt`, as well as `CUDA 12.1` and `cuML 25.06.00` for UMAP and HDBSCAN training. Core dependencies include:
- [`BERTopic`](https://github.com/MaartenGr/BERTopic) ‚Äî a convenient API for vizualizing generated topics, working with topic texts, and hierarchical structure;
- [`PyTorch`](https://github.com/pytorch/pytorch) ‚Äî to speed up the process of embedding extraction and mean pooling;
- [`Transformers`](https://github.com/huggingface/transformers) & [`SBERT`](https://sbert.net/) ‚Äî to speed up the process of embedding extraction and mean pooling;
- [`alpha_vantage`](https://github.com/RomelTorres/alpha_vantage) ‚Äî API for collecting minute-long OHLCV data;
- [`Polars`](https://docs.pola.rs/) ‚Äî for lazy data preprocessing.


## üå≥ Project Structure
<details>
<summary>
FinABYSS
</summary>

```bash
‚îú‚îÄ‚îÄ data/
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ preprocessed/
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ articles.parquet    # Fully preprocessed articles by texts and its metadata
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ embeddings_mp.npy   # Mean pooled embeddings for each article in the corpus
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ raw/
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ articles.parquet    # Parsed news articles with their metadata
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ news_urls.parquet   # Scrapped urls leading to rhe news pages on Yahoo! Finance
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ ohlcv.parquet
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ sample/                 # Training sample of 126.000 articles + their embeddings
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ articles.parquet
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ embeddings.npy
‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ embeddings_l2.npy
‚îú‚îÄ‚îÄ docs/                       # Images and gifs for README
‚îú‚îÄ‚îÄ notebooks/
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ 01_data_collecting/     # Data collecting pipelines
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ ohlcv.ipynb
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ yahoo_articles.ipynb
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ 02_data_preprocessing/  # Data prep, viz, feature extraction and sampling pipelines
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ img/
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ 01_articles_preprocessing.ipynb
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ 02_articles_vizualization.ipynb
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ 03_feature_extraction.ipynb
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ 04_data_sampling.ipynb
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ 03_topic_modeling/
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ img/                # Images from model analysis and topic viz (L2-based model)
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ models/
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ v1/             # Exploratory first version of topic model
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ v2/             # Topic model based on L2-Euclidean dist metric
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ v3/             # Topic model based on Cosine & Euclidean dist metrics
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ summary.md
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ 01_hpo.ipynb
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ 02_topic_modeing.ipynb
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ 03_model_analysis.ipynb
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ 04_topic_vizualization.ipynb
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ 04_semantic_map_dev/
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ data/                # Embeddings from the intermediate space and 2D + labels
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ semantic_map.ipynb   # HTML/CSS/JS + Python wrapper for Semantic Map
‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ semantic_map_l2.html # Semantic Map based on L2-Euclidean dist metric
‚îú‚îÄ‚îÄ paper/
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ eng/    # English thesis with bibliography and LaTeX
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ ru/     # Russian version of thesis (badly adapted)
‚îú‚îÄ‚îÄ parsers/    # Still class for parsing URLs and news from only Yahoo! Finance
‚îú‚îÄ‚îÄ utils/
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ api_key_manager.py     # API-key rotator class
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ custom_tqdm.py         # TQDM-wrapper for monitoring parsing process
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ metrics.py             # Metrics for HPO (finally unused)
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ proxy_manager.py       # Proxy-rotator for parsing articles
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ pruner.py              # 3 pruners for HPO with Optuna
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ vizualization.py       # Functions for viz parallel coordinates & feature importance
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ semantic_map.html # Main Semantic Map based on Cosine + Euclidean dist metric
```
</details>

## üöß Future Works
| Task                                                                           | Complexity | Priority | Current Status | Finished  |
|--------------------------------------------------------------------------------|------------|----------|----------------|-----------|
| ~~Additional Corpus Cleaning~~                                                     | Easy       | High     | done           | &#x2611; Formalize rules
| ~~Improve c-TF-IDF implementation~~                                                | Easy       | High     | done           | &#x2611; Configure MinimalMarginalRate from KeyBERT
| ~~Improve Representation of Topics~~                                               | Normal     | High     | done           | &#x2611; Plug in the GPT-4o<br>&#x2611; Figure out how to account for the hierarchical structure
| ~~Develop Hierarchical Structure Processing Logic Inspiring by BERTopic~~          | Normal     | Medium   | in progress    | &#x2611; Explore the BERTopic source code<br>&#x2611; Develop logic for assigning topic names to each level in the hierarchy
| Full-body Training (all corpus)                                                             | Normal     | High     | done           | &#x2610; Find and rent infrastructure with GPUs<br>&#x2610; Customize the environment<br>&#x2610; Adapt training code
| ModernBERT Domain Adaptive Pre-Training                                        | Hard       | High     | planning       | &#x2610; (optional) Expand the corpus<br>&#x2610; Find and rent infrastructure with GPUs<br>&#x2610; Write training code
| Evaluate and compare ModernBERT (DAPT) & FinBERT on GLUE & FLUE benchmarks     | Hard       | High     | planning       | &#x2610; Find and gather all datasets from benchmarks<br>&#x2610; Write code to fine-tune for each of the tasks
| ~~Refine the Semantic Map~~                                                        | Normal     | Medium   | done       | &#x2611; Rework hover labels<br>&#x2611; Write custom code to build a word cloud using the native TF-IDF<br>&#x2611; Implement filtering by source<br>&#x2611; Improve the visual design<br>&#x2611; Develop more diverse infographics
| Refine the Text Search in the Semantic Map                                     | Normal     | Low      | planning       | &#x2610; Create the vector DB (FAISS) for texts<br>&#x2611; Remove the texts from hover labels and migrate them into the database<br>&#x2610; Connect search system in the semantic map wit database
| Fine-tuning ModernBERT (DAPT) for Dense Embeddings                                    | Hard       | High     | planning       | &#x2610; ...
| Experimentation with Leiden –°lustering on kNN Graphs                           | Normal     | Low      | backlog        | &#x2610; ...
| Companys' Semantic Graph                                                       | Hard       | Low      | backlog        | &#x2610; ...
| Graph-based News Representation                                                | Hard       | Low      | backlog        | &#x2610; ...

## üìû Contacts

Email: denis.tomin.alpino@gmail.com

Telegram: [@denisalpino](https://t.me/denisalpino)